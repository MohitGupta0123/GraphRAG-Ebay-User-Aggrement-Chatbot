{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12229081,"sourceType":"datasetVersion","datasetId":7704748}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"e1aef41f-ee65-4844-80b1-8832dca6c52f","cell_type":"code","source":"!pip install dotenv neo4j","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:29:37.410645Z","iopub.execute_input":"2025-06-21T05:29:37.410929Z","iopub.status.idle":"2025-06-21T05:29:41.732118Z","shell.execute_reply.started":"2025-06-21T05:29:37.410906Z","shell.execute_reply":"2025-06-21T05:29:41.731219Z"}},"outputs":[{"name":"stdout","text":"Collecting dotenv\n  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\nCollecting neo4j\n  Downloading neo4j-5.28.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting python-dotenv (from dotenv)\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from neo4j) (2025.2)\nDownloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\nDownloading neo4j-5.28.1-py3-none-any.whl (312 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.3/312.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nInstalling collected packages: python-dotenv, neo4j, dotenv\nSuccessfully installed dotenv-0.9.9 neo4j-5.28.1 python-dotenv-1.1.0\n","output_type":"stream"}],"execution_count":1},{"id":"443d86eb","cell_type":"code","source":"from dotenv import load_dotenv\nimport os\n\n# Load environment variables from .env file\nload_dotenv(dotenv_path=r\"/kaggle/input/secrets/.env\")\n\n# Retrieve the Hugging Face token\nHF_TOKEN = os.getenv(\"HF_TOKEN\")\nif not HF_TOKEN:\n    raise ValueError(\"HF_TOKEN not found in environment variables\")\nelse:\n    print(\"HF_TOKEN loaded successfully\")\n\n# Neo4j Credentials\nNEO4J_URI = os.getenv(\"NEO4J_URI\")\nNEO4J_USERNAME = os.getenv(\"NEO4J_USERNAME\")\nNEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\")\nNEO4J_DATABASE = os.getenv(\"NEO4J_DATABASE\", \"neo4j\")\n\n# Check if all are loaded\nif not all([NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD, NEO4J_DATABASE]):\n    raise ValueError(\"One or more Neo4j credentials are missing in environment variables\")\n\nprint(\"Neo4j credentials loaded successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:29:41.733564Z","iopub.execute_input":"2025-06-21T05:29:41.733843Z","iopub.status.idle":"2025-06-21T05:29:41.758234Z","shell.execute_reply.started":"2025-06-21T05:29:41.733812Z","shell.execute_reply":"2025-06-21T05:29:41.757666Z"}},"outputs":[{"name":"stdout","text":"HF_TOKEN loaded successfully\nNeo4j credentials loaded successfully\n","output_type":"stream"}],"execution_count":2},{"id":"829272ab","cell_type":"code","source":"# user_query = \"What are eBay's listing requirements?\"\n# user_query = \"What laws govern the eBay User Agreement for U.S. users?\"\nuser_query = \"What ebay may terminate?\"\n# user_query = \"Tell about Ebay servies?\"\n\nentities_in_query = user_query.split()\n\nprint(\"Entities in user query:\", entities_in_query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:29:41.758911Z","iopub.execute_input":"2025-06-21T05:29:41.759143Z","iopub.status.idle":"2025-06-21T05:29:41.763442Z","shell.execute_reply.started":"2025-06-21T05:29:41.759127Z","shell.execute_reply":"2025-06-21T05:29:41.762724Z"}},"outputs":[{"name":"stdout","text":"Entities in user query: ['What', 'ebay', 'may', 'terminate?']\n","output_type":"stream"}],"execution_count":3},{"id":"489c59d9","cell_type":"code","source":"from neo4j import GraphDatabase\n\ndriver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n\ndef retrieve_relevant_triplets(entities):\n    # query = \"\"\"\n    # MATCH (a)-[r]->(b)\n    # WHERE a.name IN $entities OR b.name IN $entities\n    # RETURN a.name AS subject, type(r) AS relation, b.name AS object\n    # LIMIT 10\n    # \"\"\"\n    # query = \"\"\"\n    # MATCH (a)-[r]->(b)\n    # WHERE ANY(e IN $entities WHERE a.name CONTAINS e OR b.name CONTAINS e)\n    # RETURN a.name AS subject, type(r) AS relation, b.name AS object\n    # LIMIT 30\n    # \"\"\"\n    query = \"\"\"\n    MATCH (a)-[r]->(b)\n        WHERE ANY(e IN $entities WHERE toLower(a.name) CONTAINS toLower(e) OR toLower(b.name) CONTAINS toLower(e))\n        RETURN a.name AS subject, type(r) AS relation, b.name AS object\n        LIMIT 30\n    \"\"\"\n    with driver.session() as session:\n        result = session.run(query, entities=entities)\n        return [f\"{row['subject']} {row['relation']} {row['object']}\" for row in result]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:29:41.764755Z","iopub.execute_input":"2025-06-21T05:29:41.764993Z","iopub.status.idle":"2025-06-21T05:29:42.268538Z","shell.execute_reply.started":"2025-06-21T05:29:41.764977Z","shell.execute_reply":"2025-06-21T05:29:42.268019Z"}},"outputs":[],"execution_count":4},{"id":"5938ffe3","cell_type":"code","source":"# import torch\n# from transformers import pipeline\n\n# # Initialize the LLaMA 3.2B Instruct model for chat-style input\n# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n# llm = pipeline(\n#     \"text-generation\",\n#     model=model_id,\n#     torch_dtype=torch.bfloat16,\n#     device_map=\"cuda\",\n# )\n\n# # Step 1: Get relevant graph facts\n# triplets = retrieve_relevant_triplets(entities_in_query)\n\n# # Step 2: Build a context-aware question prompt\n# def build_prompt(context_triplets, question):\n#     # if not context_triplets:\n#     #     context = \"No relevant facts were found in the knowledge graph.\"\n#     # else:\n#     context = \"\\n\".join(f\"- {triplet}\" for triplet in context_triplets)\n#     print(context)\n    \n#     return f\"\"\"Answer the user's question strictly using the knowledge graph facts provided.\n# If the answer is not directly supported by the facts, respond with \"I don't have enough information to answer that.\"\n# And tell what you can tell\n# Facts:\n# {context}\n\n# Question: {question}\n# Answer:\"\"\"\n\n# prompt = build_prompt(triplets, user_query)\n\n# # Step 3: Wrap the prompt into chat-style input\n# messages = [\n#     {\n#         \"role\": \"system\",\n#         \"content\": (\n#             \"You are a precise assistant specialized in answering questions using knowledge graph data. \"\n#             \"You only use the facts provided and avoid speculation.\"\n#         ),\n#     },\n#     {\"role\": \"user\", \"content\": prompt},\n# ]\n\n# # Step 4: Run the model\n# outputs = llm(messages, max_new_tokens=200, do_sample=False)\n\n# # Step 5: Extract and print response\n# response_text = outputs[0][\"generated_text\"]\n# cleaned_response = \"\\n\\nAnswer: \" + response_text[2]['content'].split(\"Answer:\")[-1].strip()\n# print(cleaned_response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:29:42.269542Z","iopub.execute_input":"2025-06-21T05:29:42.269960Z","iopub.status.idle":"2025-06-21T05:29:42.274173Z","shell.execute_reply.started":"2025-06-21T05:29:42.269940Z","shell.execute_reply":"2025-06-21T05:29:42.273373Z"}},"outputs":[],"execution_count":5},{"id":"195e5ec6-b6a6-4b38-88c5-0527c1a447e6","cell_type":"code","source":"driver.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:29:44.910327Z","iopub.execute_input":"2025-06-21T05:29:44.910978Z","iopub.status.idle":"2025-06-21T05:29:44.914465Z","shell.execute_reply.started":"2025-06-21T05:29:44.910953Z","shell.execute_reply":"2025-06-21T05:29:44.913733Z"}},"outputs":[],"execution_count":6},{"id":"97b3a277-d44e-48ea-b1cb-0e12dc3fbd93","cell_type":"code","source":"import os\nimport json\nimport requests\nfrom neo4j import GraphDatabase\n\n# -- Neo4j Setup --\nNEO4J_URI = os.environ[\"NEO4J_URI\"]\nNEO4J_USERNAME = os.environ[\"NEO4J_USERNAME\"]\nNEO4J_PASSWORD = os.environ[\"NEO4J_PASSWORD\"]\ndriver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n\ndef retrieve_relevant_triplets(entities):\n    query = \"\"\"\n    MATCH (a)-[r]->(b)\n    WHERE ANY(e IN $entities WHERE toLower(a.name) CONTAINS toLower(e) OR toLower(b.name) CONTAINS toLower(e))\n    RETURN a.name AS subject, type(r) AS relation, b.name AS object\n    LIMIT 30\n    \"\"\"\n    with driver.session() as session:\n        result = session.run(query, entities=entities)\n        return [f\"{row['subject']} {row['relation']} {row['object']}\" for row in result]\n\n# -- Prompt Construction --\ndef build_prompt(context_triplets, question):\n    context = \"\\n\".join(f\"- {triplet}\" for triplet in context_triplets) if context_triplets else \"No relevant facts were found in the knowledge graph.\"\n    return f\"\"\"Answer the user's question strictly using the knowledge graph facts provided.\nIf the answer is not directly supported by the facts, respond with \"I don't have enough information to answer that.\"\nAnd tell what you can tell.\nFacts:\n{context}\n\nQuestion: {question}\nAnswer:\"\"\"\n\n# -- Hugging Face Streaming Chat Completion --\nAPI_URL = \"https://router.huggingface.co/sambanova/v1/chat/completions\"\nheaders = {\n    \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n}\n\ndef query_llm(messages):\n    payload = {\n        \"model\": \"Meta-Llama-3.2-3B-Instruct\",\n        \"messages\": messages,\n        \"stream\": True,\n    }\n    response = requests.post(API_URL, headers=headers, json=payload, stream=True)\n    for line in response.iter_lines():\n        if not line.startswith(b\"data:\"):\n            continue\n        if line.strip() == b\"data: [DONE]\":\n            return\n        yield json.loads(line.decode(\"utf-8\").lstrip(\"data:\").rstrip(\"/n\"))\n\n# -- Main Workflow --\nif __name__ == \"__main__\":\n    user_query = \"What ebay may terminate?\"\n    entities_in_query = user_query.split()  # Simple token-based entity extraction\n\n    print(\"Entities in user query:\", entities_in_query)\n\n    # Step 1: Get KG triplets\n    triplets = retrieve_relevant_triplets(entities_in_query)\n\n    # Step 2: Build prompt\n    prompt = build_prompt(triplets, user_query)\n\n    # Step 3: Prepare chat-style input\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": (\n                \"You are a precise assistant specialized in answering questions using knowledge graph data. \"\n                \"You only use the facts provided and avoid speculation.\"\n            ),\n        },\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n\n    # Step 4: Query LLM and stream response\n    print(\"\\nAnswer:\", end=\" \", flush=True)\n    for chunk in query_llm(messages):\n        content = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n        print(content, end=\"\", flush=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T05:29:54.630485Z","iopub.execute_input":"2025-06-21T05:29:54.630955Z","iopub.status.idle":"2025-06-21T05:29:56.839283Z","shell.execute_reply.started":"2025-06-21T05:29:54.630931Z","shell.execute_reply":"2025-06-21T05:29:56.838803Z"}},"outputs":[{"name":"stdout","text":"Entities in user query: ['What', 'ebay', 'may', 'terminate?']\n\nAnswer: According to the facts, eBay may terminate:\n\n1. Our Services\n2. Anyone\n3. This Agreement\n4. At any time\n5. With notice\n6. By giving notice\n7. The Services","output_type":"stream"}],"execution_count":8},{"id":"771ef732-f2c2-4156-bf1c-decf2947ce8d","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}